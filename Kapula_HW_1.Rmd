---
title: "Hwk #1:  Regression"
author: "Ntemena Kapula"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For this homework we will use NHANES data that exists in a package for R.

NHANES consists of survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960's. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination center (MEC).

Note that there is the following warning on the NHANES website:
“For NHANES datasets, the use of sampling weights and sample design variables is recommended for all analyses because the sample design is a clustered design and incorporates differential probabilities of selection. If you fail to account for the sampling parameters, you may obtain biased estimates and overstate significance levels.”

For this homework, please ignore this warning and just apply our analyses to the data as if they were randomly sampled! We will be using the data called `NHANESraw`.

For questions that ask for your comments, it suffices to answer with one or two sentences in each case.

## Data Preparation

1. Install the package `NHANES` into R, load the `NHANES` package, and then run the command `data(NHANES)` which will load the NHANES data. Type `?NHANES` and read about the dataset.
```{r}
# install.packages("NHANES")
library(NHANES)

data(NHANES)

?NHANES

```

2. Make an object `nhanes` that is a subset version `NHANESraw` that does not include any missing data for `Diabetes`, `BPSysAve`, `BPDiaAve`

```{r}
nhanes <- subset(NHANESraw, complete.cases(Diabetes, BPSysAve, BPDiaAve))
# professor code (pc): nhanes <- NHANESraw %>% filter(!is.na(Diabetes) & !is.na(BPSysAVe) & !is.na(BPDiaAve))
```

3. (1 point) Plot `BPSysAve` against `BPDiaAve`. Comment on what is going on.
Further subset the data such the observations with `BPDiaAve` equal to zero are removed.
```{r}

plot(nhanes$BPDiaAve, nhanes$BPSysAve, pch=16, cex=0.5, xlab = "BPDiaAve", ylab = "BPSysAve")
#pc: plot(nhanes$BPDiaAve,nhanes$BPSysAve,pch=19, cex=0.2, xlab="Diastolic BP", ylab="Systolic BP")

# The plot below shows point of BPDiasysAve against BPDiaAve with most of the point falling within the BPSysAve of approximately 95 to 160 and BPDiaAve of approximately 35 to 95. we see a few outliers beyond 200 BPSysAve and 120 BPDiaAve

nhanes_BPD <- subset(nhanes, BPDiaAve > 0)
# pc: nhanesnozeros <- nhanes %>% filter(BPDiaAve != 0)

plot(nhanes_BPD$BPDiaAve, nhanes_BPD$BPSysAve, pch=16, cex=0.5, xlab = "BPDiaAve", ylab = "BPSysAve")

```

4. (1 point)
Make an object `nhanes09` that is a subset of `nhanes` to only the 2009_10 data. This will be your training dataset. Also make an object `nhanes11` that is a subset of `nhanes` to only the 2011_12 data. This will be your test dataset.
```{r}

nhanes09 <- subset(nhanes_BPD, SurveyYr == '2009_10') # training set

nhanes11 <- subset(nhanes_BPD, SurveyYr == '2011_12') # testing set

#pc: 

nhanes09 <- nhanesnozeros %>% filter(SurveyYr == "2009_10")
nhanes11 <- nhanesnozeros %>% filter(SurveyYr == "2011_12")
#rm(nhanesnozeros)  # to save memory
#rm(nhanes)  # to save memory

```

## Linear regression

5. (1 point) Plot `BPSysAve` against `BPDiaAve` for the training data.
```{r}
plot(nhanes09$BPDiaAve, nhanes09$BPSysAve, pch=16, cex=0.4, xlab = "BPDiaAve", ylab = "BPSysAve")

#pc: 
plot(nhanes09$BPDiaAve,nhanes09$BPSysAve,pch=19, cex=0.2, xlab="Diastolic BP", ylab="Systolic BP")
```

6. (2 points) Fit a linear model using the training data with `BPSysAve` as outcome and `BPDiaAve` as the single predictor.
```{r}
lm1 <- lm(BPSysAve ~ BPDiaAve, data = nhanes09)

#pc:
lm1 <- lm(BPSysAve ~ BPDiaAve, data=nhanes09)
```

7. (1 point) Use the `summary` command to examine the resulting fitted model.
```{r}
summary(lm1)
```

8. (1 point) Generate 95% confidence intervals for the parameters of the fitted model.
```{r}
confint(lm1, level=0.95)

#pc: 
library(stats)
confint(lm1)
```

9. (1 point) Also, generate 99% confidence intervals for the parameters of the fitted model.
```{r}
confint(lm1, level=0.99)

```

10. (2 points) Comment on the difference between the 95% and 99% confidence intervals and whether or not the difference is what you would expect.

```{r}
# The 95% and 99% confidence intervals (CI) are very similar with the 95% CI being slightly narrower than the 99% CI which is expected.

# pc:
The 99% confidence intervals are wider than the 95% confidence intervals. This is expected.
```

11. (3 points) Now fit models with quadratic and cubic terms in the predictor `BPDiaAve` in addition to the linear term. Look at the output of each model with summary and generate 95% confidence intervals for each.
```{r}
BPDiaAve <- nhanes09$BPDiaAve
BPSysAve <- nhanes09$BPSysAve

lm2 <- lm(BPSysAve ~ BPDiaAve + I(BPDiaAve^2), data = nhanes09)

lm3 <- lm(BPSysAve ~ BPDiaAve + I(BPDiaAve^2) + I(BPDiaAve^3), data = nhanes09)

summary(lm2)
summary(lm3)

confint(lm2, level=0.95)
confint(lm3, level=0.95)
```

12. (2 points) Plot the training data along with the linear, quadratic and cubic fit lines in different colors.
```{r}
quad <- function(x) {predict(lm2, data.frame(BPDiaAve=x))}
cube <- function(x) {predict(lm3, data.frame(BPDiaAve=x))}

plot(BPDiaAve, BPSysAve, pch=16, cex=0.5)
abline(lm1,col="red", lwd=3)
curve(quad,add=TRUE, col="blue", lwd=3)
curve(cube,add=TRUE, col="green", lwd=3, lty=2)
legend("bottomright", cex=0.5, lwd = 3, lty=c(1,1,2), c("Linear", "Quadratic", "Cubic"), col=c("red", "blue", "green"))

#pc:

quad <- function(x) lm2$coefficients[1] + lm2$coefficients[2]*x + lm2$coefficients[3]*x^2

cube <- function(x) lm3$coefficients[1] + lm3$coefficients[2]*x + lm3$coefficients[3]*x^2 + lm3$coefficients[4]*x^3

plot(nhanes09$BPDiaAve,nhanes09$BPSysAve,pch=19, cex=0.2, xlab="Diastolic BP", ylab="Systolic BP")
abline(lm1,col=2,lwd=3)
curve(quad, add=TRUE, col=3, lwd=3)
curve(cube, add=TRUE, col=4, lwd=3, lty=2)
legend("bottomright",cex=0.8,lwd=3,c("linear","quadratic","cubic"),col=2:4)
```

13. (1 point) Which would be your preferred model based on the visual fits?

I would have preferred the quadratic model (lm2) which best fits the points on the graph.

# pc:
The quadratic or the cubic models both look reasoanble. I’d pick the cubic model since it seems to fit slightly better at the left and right ends of the plot.

14. (3 points) Perform an anova test comparing the 3 models. Does the result seem in line with what you were expecting from the visual fits? Why/why not?
```{r}
anova(lm1, lm2, lm3)

#The result seems in line with what I expected, the quadratic fit improves the linear fit significantly shown by the very small p-value of <2.2e-16. The cubic function significantly improves it as well given by the small p-value but not as significant as the quadratic model

#pc:
The ANOVA results state that the cubic model is the best fit (because the p-value for linear vs quadratic was significant and the p-value for quadratic vs cubic was also significant). This aligns with our interpretation from the plots.
```

15. (1 point) Now plot `BPSysAve` against `BPDiaAve` for the test data and overlay the fitted linear, quadratic, and cubic models.
```{r}
BPDiaAve1 <- nhanes11$BPDiaAve
BPSysAve1 <- nhanes11$BPSysAve

quad <- function(x) {predict(lm2, data.frame(BPDiaAve=x))}
cube <- function(x) {predict(lm3, data.frame(BPDiaAve=x))}

plot(BPDiaAve1, BPSysAve1, pch=16, cex=0.5)
abline(lm1,col="red", lwd=3)
curve(quad,add=TRUE, col="blue", lwd=3)
curve(cube,add=TRUE, col="green", lwd=3, lty=2)
legend("bottomright", cex=0.5, lwd = 3, lty=c(1,1,2), c("Linear", "Quadratic", "Cubic"), col=c("red", "blue", "green"))

#pc: 
plot(nhanes11$BPDiaAve,nhanes11$BPSysAve,pch=19, cex=0.2, xlab="Diastolic BP", ylab="Systolic BP")
abline(lm1,col=2,lwd=3)
curve(quad, add=TRUE, col=3, lwd=3)
curve(cube, add=TRUE, col=4, lwd=3)
legend("bottomright",cex=0.8,lwd=3,c("linear","quadratic","cubic"),col=2:4)
```

16. (3 points) Does this change your opinion at all about which is the best fit? Why/why not?

This does not change my opinion on which is the best fit, the quadratic model still appeals to fit well on the test data plot.

# pc
No, it doesn’t change my opinion. The plot on the test data looks similar to the plot on the training data.

## Smoothing kernels:
17. (3 points) Fit a nearest neighbors curve with `ksmooth` using the "normal" kernel to the `nhanes09` data with bandwidths of 3, 10, and 20.
```{r}
knormal_curve3 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 3)
knormal_curve10 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 10)
knormal_curve20 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 20)
```

18. (2 points) Plot the test data as well as the fitted curves from kernel smoothing and the best model fitted in the previous section using linear regression. Use different colors for each model.
```{r}
plot(BPDiaAve1, BPSysAve1, pch=16, cex=0.5, xlab = "BPDiaAve", ylab="BPSysAve")
lines(knormal_curve3$x, knormal_curve3$y, col="yellowgreen", lwd=3)
lines(knormal_curve10$x, knormal_curve10$y, col="red", lwd=3)
lines(knormal_curve20$x, knormal_curve20$y, col="green", lwd=3, lty=3)
curve(quad,add=TRUE, col="blue", lwd=3)
legend("bottomright",cex=0.8,lwd=3,c("best_model","normal bw=3","normal bw=10","normal bw=20"),col=c("blue", "yellowgreen", "red", "green"),lty = c(1,1,2,3))
```

19. (1 point) Based on the above results, which model would you pick? In the next section, we'll evaluate the model based on its test error.

I would select the quadratic regression model based on the graph above as it is the simplest model

#pc:
Among the kernel smoothing fits, the one with a kernel width of 20 seems the most reasonable. The one with kernel width of 3 is too wiggly and seems to be overfitting to the data. Given that the cubic model is very similar to the kernel smoother with kernel width 20, I’d prefer the cubic model as it is the simpler model.

## Evaluating error on a test set
20. (5 points) Evaluate the mean squared error of the fitted models from the "Linear Regression" section on the test data. Also provide standard errors.
```{r}
# Predicted values based on models fitted on training data

test_lm1 <- predict(lm1, nhanes11)
test_lm2 <- predict(lm2, nhanes11)
test_lm3 <- predict(lm3, nhanes11)

# Mean Squared error of each fitted model on the test data
BPSysAve11 <- nhanes11$BPSysAve

(mse1 <- mean((BPSysAve11-test_lm1)^2))
(mse2 <- mean((BPSysAve11-test_lm2)^2))
(mse3 <- mean((BPSysAve11-test_lm3)^2))

# Standard Errors
(sd1 <- sd((BPSysAve11-test_lm1)^2))
(sd2 <- sd((BPSysAve11-test_lm2)^2))
(sd3 <- sd((BPSysAve11-test_lm3)^2))

#library(rtemis) # another option for calculating mse using rtemis package
#(mse_test1 <- mse(BPSysAve11,test_lm1)) 
#(mse_test2 <- mse(BPSysAve11,test_lm2))
#(mse_test3 <- mse(BPSysAve11,test_lm3))

#pc:
linear_pred <- predict(lm1, nhanes11)
quad_pred <- predict(lm2, nhanes11)
cube_pred <- predict(lm3, nhanes11)
linear_err <- (nhanes11$BPSysAve - linear_pred)^2
quad_err <- (nhanes11$BPSysAve - quad_pred)^2
cube_err <- (nhanes11$BPSysAve - cube_pred)^2
test_mses <- c(
  mean(linear_err),
  mean(quad_err),
  mean(cube_err))
test_SEs <- c(
  sqrt(var(linear_err)/length(linear_err)),
  sqrt(var(quad_err)/length(quad_err)),
  sqrt(var(cube_err)/length(cube_err)))
data.frame(
  model_type=c("linear", "quad", "cube"),
  mse=test_mses,
  se=test_SEs)
```

21. (2 points) Using ANOVA, did we pick the model with the lowest mean squared error on the test data? Why do you think this happened?

Yes, I picked the quadratic model (lm2) which resulted in the lowest mean squared error on the test data. This may have happened because the quadratic model is the best fit line in the test data as well and so sum of residuals is the lowest in the quadratic model resulting in the smallest mean squared loss relative to other models

# pc:
The quadatic model achieved the smallest test MSE. This is not the model we picked using ANOVA. However, we see that the test error of the quadratic model is within one standard error of the cubic model. So there isn’t enough test data to definitively determine which model is better. If the quadratic model is truly better, it could be due to distributional shifts in the data. That is, it wouldn’t be surprising that ANOVA on the 2009-2010 data gives a different result from the test error on the 2011-2012 data. Empirically, when there are distributional shifts in the data, simpler models tend to perform better.

22. (5 points) Evaluate the mean squared error of the fitted models from kernel smoothing on the test data.
```{r warning=FALSE, message=FALSE}
library(rtemis)
k_curve3 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 3, x.points = nhanes11$BPDiaAve)
k_curve10 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 10, x.points = nhanes11$BPDiaAve)
k_curve20 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 20, x.points = nhanes11$BPDiaAve)


(mse_k3 <- mse(BPSysAve11,k_curve3$y))
(mse_k10 <- mse(BPSysAve11,k_curve10$y))
(mse_k20 <- mse(BPSysAve11,k_curve20$y))

# pc

nhanes11_sort <- arrange(nhanes11, BPDiaAve)
ksmooth3_test <- ksmooth(nhanes09$BPDiaAve,nhanes09$BPSysAve,kernel="normal",bandwidth=3, x.points = nhanes11_sort$BPDiaAve)
ksmooth10_test <- ksmooth(nhanes09$BPDiaAve,nhanes09$BPSysAve,kernel="normal",bandwidth=10, x.points = nhanes11_sort$BPDiaAve)
ksmooth20_test <- ksmooth(nhanes09$BPDiaAve,nhanes09$BPSysAve,kernel="normal",bandwidth=20, x.points = nhanes11_sort$BPDiaAve)
ksmooth3_err <- (nhanes11_sort$BPSysAve - ksmooth3_test$y)^2
ksmooth10_err <- (nhanes11_sort$BPSysAve - ksmooth10_test$y)^2
ksmooth20_err <- (nhanes11_sort$BPSysAve - ksmooth20_test$y)^2
test_mses <- c(
  mean(ksmooth3_err),
  mean(ksmooth10_err),
  mean(ksmooth20_err))
test_SEs <- c(
  sqrt(var(ksmooth3_err)/length(ksmooth3_err)),
  sqrt(var(ksmooth10_err)/length(ksmooth10_err)),
  sqrt(var(ksmooth20_err)/length(ksmooth20_err)))
data.frame(
  bw=c(3,10,20),
  mse=test_mses,
  SE=test_SEs)
```

23. (1 point) Among all the methods we tried, which one had the lowest test error? Was the test error of your selected model within one standard error of the minimum test error?
```{r}
#The lowest test error was obtained from the quadratic model with mse of 249.99 and standard error of 577.83. The best model selected had the mean of the minimum test error which is within the one standard error of the minimum test error.

#pc: 
The quadratic model got the lowest test error. We had selected the cubic model and it is within one standard error of the minimum test error.

```
