---
title: "Hwk #1:  Regression"
author: "Ntemena Kapula"
output: html_document

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

For this homework we will use NHANES data that exists in a package for R.

NHANES consists of survey data collected by the US National Center for Health Statistics (NCHS) which has conducted a series of health and nutrition surveys since the early 1960's. Since 1999 approximately 5,000 individuals of all ages are interviewed in their homes every year and complete the health examination component of the survey. The health examination is conducted in a mobile examination center (MEC).

Note that there is the following warning on the NHANES website:
“For NHANES datasets, the use of sampling weights and sample design variables is recommended for all analyses because the sample design is a clustered design and incorporates differential probabilities of selection. If you fail to account for the sampling parameters, you may obtain biased estimates and overstate significance levels.”

For this homework, please ignore this warning and just apply our analyses to the data as if they were randomly sampled! We will be using the data called `NHANESraw`.

For questions that ask for your comments, it suffices to answer with one or two sentences in each case.

## Data Preparation

1. Install the package `NHANES` into R, load the `NHANES` package, and then run the command `data(NHANES)` which will load the NHANES data. Type `?NHANES` and read about the dataset.
```{r}
# install.packages("NHANES")
library(NHANES)

data(NHANES)

?NHANES

```

2. Make an object `nhanes` that is a subset version `NHANESraw` that does not include any missing data for `Diabetes`, `BPSysAve`, `BPDiaAve`

```{r}
nhanes <- subset(NHANESraw, complete.cases(Diabetes, BPSysAve, BPDiaAve))

```

3. (1 point) Plot `BPSysAve` against `BPDiaAve`. Comment on what is going on.
Further subset the data such the observations with `BPDiaAve` equal to zero are removed.
```{r}

plot(nhanes$BPDiaAve, nhanes$BPSysAve, pch=16, cex=0.5, xlab = "BPDiaAve", ylab = "BPSysAve")

# The plot below shows point of BPDiasysAve against BPDiaAve with most of the point falling within the BPSysAve of approximately 95 to 160 and BPDiaAve of approximately 35 to 95. we see a few outliers beyond 200 BPSysAve and 120 BPDiaAve

nhanes_BPD <- subset(nhanes, BPDiaAve > 0)

plot(nhanes_BPD$BPDiaAve, nhanes_BPD$BPSysAve, pch=16, cex=0.5, xlab = "BPDiaAve", ylab = "BPSysAve")

```

4. (1 point)
Make an object `nhanes09` that is a subset of `nhanes` to only the 2009_10 data. This will be your training dataset. Also make an object `nhanes11` that is a subset of `nhanes` to only the 2011_12 data. This will be your test dataset.
```{r}

nhanes09 <- subset(nhanes_BPD, SurveyYr == '2009_10') # training set

nhanes11 <- subset(nhanes_BPD, SurveyYr == '2011_12') # testing set

```

## Linear regression

5. (1 point) Plot `BPSysAve` against `BPDiaAve` for the training data.
```{r}
plot(nhanes09$BPDiaAve, nhanes09$BPSysAve, pch=16, cex=0.4, xlab = "BPDiaAve", ylab = "BPSysAve")
```

6. (2 points) Fit a linear model using the training data with `BPSysAve` as outcome and `BPDiaAve` as the single predictor.
```{r}
lm1 <- lm(BPSysAve ~ BPDiaAve, data = nhanes09)
```

7. (1 point) Use the `summary` command to examine the resulting fitted model.
```{r}
summary(lm1)
```

8. (1 point) Generate 95% confidence intervals for the parameters of the fitted model.
```{r}
confint(lm1, level=0.95)
```

9. (1 point) Also, generate 99% confidence intervals for the parameters of the fitted model.
```{r}
confint(lm1, level=0.99)
```

10. (2 points) Comment on the difference between the 95% and 99% confidence intervals and whether or not the difference is what you would expect.

```{r}
# The 95% and 99% confidence intervals (CI) are very similar with the 95% CI being slightly narrower than the 99% CI which is expected.
```


11. (3 points) Now fit models with quadratic and cubic terms in the predictor `BPDiaAve` in addition to the linear term. Look at the output of each model with summary and generate 95% confidence intervals for each.
```{r}
BPDiaAve <- nhanes09$BPDiaAve
BPSysAve <- nhanes09$BPSysAve

lm2 <- lm(BPSysAve ~ BPDiaAve + I(BPDiaAve^2), data = nhanes09)

lm3 <- lm(BPSysAve ~ BPDiaAve + I(BPDiaAve^2) + I(BPDiaAve^3), data = nhanes09)

summary(lm2)
summary(lm3)

confint(lm2, level=0.95)
confint(lm3, level=0.95)
```

12. (2 points) Plot the training data along with the linear, quadratic and cubic fit lines in different colors.
```{r}
quad <- function(x) {predict(lm2, data.frame(BPDiaAve=x))}
cube <- function(x) {predict(lm3, data.frame(BPDiaAve=x))}

plot(BPDiaAve, BPSysAve, pch=16, cex=0.5)
abline(lm1,col="red", lwd=3)
curve(quad,add=TRUE, col="blue", lwd=3)
curve(cube,add=TRUE, col="green", lwd=3, lty=2)
legend("bottomright", cex=0.5, lwd = 3, lty=c(1,1,2), c("Linear", "Quadratic", "Cubic"), col=c("red", "blue", "green"))
```

13. (1 point) Which would be your preferred model based on the visual fits?

I would have preferred the quadratic model (lm2) which best fits the points on the graph.

14. (3 points) Perform an anova test comparing the 3 models. Does the result seem in line with what you were expecting from the visual fits? Why/why not?
```{r}
anova(lm1, lm2, lm3)

#The result seems in line with what I expected, the quadratic fit improves the linear fit significantly shown by the very small p-value of <2.2e-16. The cubic function significantly improves it as well given by the small p-value but now as significant as the quadratic model
```

15. (1 point) Now plot `BPSysAve` against `BPDiaAve` for the test data and overlay the fitted linear, quadratic, and cubic models.
```{r}
BPDiaAve1 <- nhanes11$BPDiaAve
BPSysAve1 <- nhanes11$BPSysAve

quad <- function(x) {predict(lm2, data.frame(BPDiaAve=x))}
cube <- function(x) {predict(lm3, data.frame(BPDiaAve=x))}

plot(BPDiaAve1, BPSysAve1, pch=16, cex=0.5)
abline(lm1,col="red", lwd=3)
curve(quad,add=TRUE, col="blue", lwd=3)
curve(cube,add=TRUE, col="green", lwd=3, lty=2)
legend("bottomright", cex=0.5, lwd = 3, lty=c(1,1,2), c("Linear", "Quadratic", "Cubic"), col=c("red", "blue", "green"))
```

16. (3 points) Does this change your opinion at all about which is the best fit? Why/why not?

This does not change my opinion on which is the best fit, the quadratic model still appeals to fit well on the test data plot.

## Smoothing kernels:
17. (3 points) Fit a nearest neighbors curve with `ksmooth` using the "normal" kernel to the `nhanes09` data with bandwidths of 3, 10, and 20.
```{r}
knormal_curve3 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 3)
knormal_curve10 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 10)
knormal_curve20 <- ksmooth(nhanes09$BPDiaAve, nhanes09$BPSysAve, kernel = "normal", bandwidth = 20)
```

18. (2 points) Plot the test data as well as the fitted curves from kernel smoothing and the best model fitted in the previous section using linear regression. Use different colors for each model.
```{r}
plot(BPDiaAve1, BPSysAve1, pch=16, cex=0.5, xlab = "BPDiaAve", ylab="BPSysAve")
lines(knormal_curve3$x, knormal_curve3$y, col="yellowgreen", lwd=3)
lines(knormal_curve10$x, knormal_curve10$y, col="red", lwd=3)
lines(knormal_curve20$x, knormal_curve20$y, col="green", lwd=3, lty=3)
curve(quad,add=TRUE, col="blue", lwd=3)
legend("bottomright",cex=0.8,lwd=3,c("best_model","normal bw=3","normal bw=10","normal bw=20"),col=c("blue", "yellowgreen", "red", "green"),lty = c(1,1,2,3))
```

19. (1 point) Based on the above results, which model would you pick? In the next section, we'll evaluate the model based on its test error.

I would select the quadratic regression model based on the graph above.

## Evaluating error on a test set
20. (5 points) Evaluate the mean squared error of the fitted models from the "Linear Regression" section on the test data. Also provide standard errors.
```{r}
# Predicted values based on models fitted on training data

test_lm1 <- predict(lm1, nhanes11)
test_lm2 <- predict(lm2, nhanes11)
test_lm3 <- predict(lm3, nhanes11)

# Mean Squared error of each fitted model on the test data
BPSysAve11 <- nhanes11$BPSysAve

(mse1 <- mean((BPSysAve11-test_lm1)^2))
(mse2 <- mean((BPSysAve11-test_lm2)^2))
(mse3 <- mean((BPSysAve11-test_lm3)^2))

# Standard Errors
(sd1 <- sd((BPSysAve11-test_lm1)^2))
(sd2 <- sd((BPSysAve11-test_lm2)^2))
(sd3 <- sd((BPSysAve11-test_lm3)^2))

#library(rtemis) # another option for calculating mse using rtemis package
#(mse_test1 <- mse(BPSysAve11,test_lm1)) 
#(mse_test2 <- mse(BPSysAve11,test_lm2))
#(mse_test3 <- mse(BPSysAve11,test_lm3))

```

21. (2 points) Using ANOVA, did we pick the model with the lowest mean squared error on the test data? Why do you think this happened?

Yes, I picked the quadratic model (lm2) which resulted in the lowest mean squared error on the test data. This may have happened because the quadratic model is the best fit line in the test data as well and so sum of residuals is the lowest in the quadratic model resulting in the smallest mean squared loss relative to other models

22. (5 points) Evaluate the mean squared error of the fitted models from kernel smoothing on the test data.
```{r warning=FALSE, message=FALSE}
library(rtemis)
(mse_k3 <- mse(BPSysAve11, knormal_curve3$y))
(mse_k10 <- mse(BPSysAve11, knormal_curve10$y))
(mse_k20 <- mse(BPSysAve11, knormal_curve20$y))

```

23. (1 point) Among all the methods we tried, which one had the lowest test error? Was the test error of your selected model within one standard error of the minimum test error?
```{r}
#The lowest test error was obtained from the predicted values based on the quadratic model with mse of 249.99 and standard error of 577.83. The best model selected had the mean of the minimun test error
```